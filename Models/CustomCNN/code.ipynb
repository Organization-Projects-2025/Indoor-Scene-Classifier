{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae07bb0-ad5f-49a1-9185-d2ccacdc975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Regenerated CSV with 12335 images\n",
      "Total images: 12335, Classes: label\n",
      "living room    2621\n",
      "dining room    2605\n",
      "bed            2445\n",
      "bath           2430\n",
      "kitchen        2234\n",
      "Name: count, dtype: int64\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_flip (RandomFlip)    (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 224, 224, 3)      0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 224, 224, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 112, 112, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 112, 112, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 56, 56, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 56, 56, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 28, 28, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 28, 28, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 28, 28, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 14, 14, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 14, 14, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 14, 14, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 14, 14, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 14, 14, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " global_average_pooling2d_11  (None, 256)              0         \n",
      " 65 (GlobalAveragePooling2D)                                     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,639,621\n",
      "Trainable params: 1,637,637\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n",
      "Epoch 1/140\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:07,413 - WARNING - Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:07,475 - WARNING - Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:07,544 - WARNING - Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:07,601 - WARNING - Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:08,160 - WARNING - Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:09,473 - WARNING - Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:09,542 - WARNING - Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:09,625 - WARNING - Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:09,697 - WARNING - Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:10,268 - WARNING - Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:12,459 - WARNING - Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:12,531 - WARNING - Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:12,609 - WARNING - Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:12,673 - WARNING - Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 03:08:13,240 - WARNING - Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 158s 953ms/step - loss: 2.6273 - accuracy: 0.3166 - val_loss: 6.8446 - val_accuracy: 0.1970 - lr: 0.0010\n",
      "Epoch 2/140\n",
      "124/124 [==============================] - 123s 978ms/step - loss: 2.2688 - accuracy: 0.3807 - val_loss: 10.0422 - val_accuracy: 0.1970 - lr: 0.0010\n",
      "Epoch 3/140\n",
      "124/124 [==============================] - 102s 819ms/step - loss: 1.9826 - accuracy: 0.4379 - val_loss: 4.6576 - val_accuracy: 0.1986 - lr: 0.0010\n",
      "Epoch 4/140\n",
      "124/124 [==============================] - 103s 833ms/step - loss: 1.7753 - accuracy: 0.4874 - val_loss: 4.7322 - val_accuracy: 0.2011 - lr: 0.0010\n",
      "Epoch 5/140\n",
      "124/124 [==============================] - 102s 820ms/step - loss: 1.6195 - accuracy: 0.5281 - val_loss: 2.8307 - val_accuracy: 0.3129 - lr: 0.0010\n",
      "Epoch 6/140\n",
      "124/124 [==============================] - 102s 821ms/step - loss: 1.5216 - accuracy: 0.5565 - val_loss: 1.8651 - val_accuracy: 0.4349 - lr: 0.0010\n",
      "Epoch 7/140\n",
      "124/124 [==============================] - 100s 804ms/step - loss: 1.4497 - accuracy: 0.5891 - val_loss: 1.5767 - val_accuracy: 0.5107 - lr: 0.0010\n",
      "Epoch 8/140\n",
      "124/124 [==============================] - 100s 803ms/step - loss: 1.4070 - accuracy: 0.6009 - val_loss: 1.6914 - val_accuracy: 0.4828 - lr: 0.0010\n",
      "Epoch 9/140\n",
      "124/124 [==============================] - 100s 804ms/step - loss: 1.3658 - accuracy: 0.6292 - val_loss: 2.2543 - val_accuracy: 0.3835 - lr: 0.0010\n",
      "Epoch 10/140\n",
      "124/124 [==============================] - 102s 818ms/step - loss: 1.3610 - accuracy: 0.6299 - val_loss: 1.6070 - val_accuracy: 0.5647 - lr: 0.0010\n",
      "Epoch 11/140\n",
      "124/124 [==============================] - 99s 801ms/step - loss: 1.3423 - accuracy: 0.6369 - val_loss: 1.9009 - val_accuracy: 0.4726 - lr: 0.0010\n",
      "Epoch 12/140\n",
      "124/124 [==============================] - 101s 815ms/step - loss: 1.3125 - accuracy: 0.6563 - val_loss: 1.5815 - val_accuracy: 0.5837 - lr: 0.0010\n",
      "Epoch 13/140\n",
      "124/124 [==============================] - 100s 805ms/step - loss: 1.3066 - accuracy: 0.6650 - val_loss: 1.3658 - val_accuracy: 0.6465 - lr: 0.0010\n",
      "Epoch 14/140\n",
      "124/124 [==============================] - 100s 809ms/step - loss: 1.2972 - accuracy: 0.6718 - val_loss: 1.8055 - val_accuracy: 0.5140 - lr: 0.0010\n",
      "Epoch 15/140\n",
      "124/124 [==============================] - 100s 806ms/step - loss: 1.2810 - accuracy: 0.6830 - val_loss: 1.4844 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Epoch 16/140\n",
      "124/124 [==============================] - 101s 814ms/step - loss: 1.2694 - accuracy: 0.6876 - val_loss: 1.4382 - val_accuracy: 0.6234 - lr: 0.0010\n",
      "Epoch 17/140\n",
      "124/124 [==============================] - 100s 806ms/step - loss: 1.2662 - accuracy: 0.6938 - val_loss: 1.4327 - val_accuracy: 0.6230 - lr: 0.0010\n",
      "Epoch 18/140\n",
      "124/124 [==============================] - 100s 803ms/step - loss: 1.2599 - accuracy: 0.7007 - val_loss: 1.4307 - val_accuracy: 0.6668 - lr: 0.0010\n",
      "Epoch 19/140\n",
      "124/124 [==============================] - 98s 794ms/step - loss: 1.2641 - accuracy: 0.6976 - val_loss: 1.6667 - val_accuracy: 0.5480 - lr: 0.0010\n",
      "Epoch 20/140\n",
      "124/124 [==============================] - 101s 811ms/step - loss: 1.1692 - accuracy: 0.7318 - val_loss: 1.1770 - val_accuracy: 0.7256 - lr: 6.0000e-04\n",
      "Epoch 21/140\n",
      "124/124 [==============================] - 100s 809ms/step - loss: 1.1118 - accuracy: 0.7517 - val_loss: 1.3201 - val_accuracy: 0.6871 - lr: 6.0000e-04\n",
      "Epoch 22/140\n",
      "124/124 [==============================] - 104s 839ms/step - loss: 1.1015 - accuracy: 0.7515 - val_loss: 1.1626 - val_accuracy: 0.7244 - lr: 6.0000e-04\n",
      "Epoch 23/140\n",
      "124/124 [==============================] - 100s 805ms/step - loss: 1.0810 - accuracy: 0.7583 - val_loss: 1.5296 - val_accuracy: 0.6352 - lr: 6.0000e-04\n",
      "Epoch 24/140\n",
      "124/124 [==============================] - 105s 847ms/step - loss: 1.0669 - accuracy: 0.7582 - val_loss: 1.1933 - val_accuracy: 0.6879 - lr: 6.0000e-04\n",
      "Epoch 25/140\n",
      "124/124 [==============================] - 108s 868ms/step - loss: 1.0657 - accuracy: 0.7580 - val_loss: 1.1888 - val_accuracy: 0.7264 - lr: 6.0000e-04\n",
      "Epoch 26/140\n",
      "124/124 [==============================] - 103s 832ms/step - loss: 1.0429 - accuracy: 0.7671 - val_loss: 1.3787 - val_accuracy: 0.6680 - lr: 6.0000e-04\n",
      "Epoch 27/140\n",
      "124/124 [==============================] - 107s 864ms/step - loss: 1.0587 - accuracy: 0.7664 - val_loss: 1.2518 - val_accuracy: 0.7009 - lr: 6.0000e-04\n",
      "Epoch 28/140\n",
      "124/124 [==============================] - 103s 829ms/step - loss: 1.0343 - accuracy: 0.7824 - val_loss: 1.2950 - val_accuracy: 0.6741 - lr: 6.0000e-04\n",
      "Epoch 29/140\n",
      "124/124 [==============================] - 102s 821ms/step - loss: 0.9588 - accuracy: 0.8015 - val_loss: 1.2240 - val_accuracy: 0.7041 - lr: 3.6000e-04\n",
      "Epoch 30/140\n",
      "124/124 [==============================] - 102s 822ms/step - loss: 0.9452 - accuracy: 0.7977 - val_loss: 1.2327 - val_accuracy: 0.7150 - lr: 3.6000e-04\n",
      "Epoch 31/140\n",
      "124/124 [==============================] - 101s 817ms/step - loss: 0.9037 - accuracy: 0.8134 - val_loss: 0.9694 - val_accuracy: 0.7904 - lr: 3.6000e-04\n",
      "Epoch 32/140\n",
      "124/124 [==============================] - 100s 804ms/step - loss: 0.8925 - accuracy: 0.8159 - val_loss: 1.2612 - val_accuracy: 0.7094 - lr: 3.6000e-04\n",
      "Epoch 33/140\n",
      "124/124 [==============================] - 100s 806ms/step - loss: 0.8886 - accuracy: 0.8167 - val_loss: 1.1370 - val_accuracy: 0.7304 - lr: 3.6000e-04\n",
      "Epoch 34/140\n",
      "124/124 [==============================] - 103s 826ms/step - loss: 0.8731 - accuracy: 0.8225 - val_loss: 1.1366 - val_accuracy: 0.7471 - lr: 3.6000e-04\n",
      "Epoch 35/140\n",
      "124/124 [==============================] - 104s 838ms/step - loss: 0.8652 - accuracy: 0.8251 - val_loss: 1.1591 - val_accuracy: 0.7317 - lr: 3.6000e-04\n",
      "Epoch 36/140\n",
      "124/124 [==============================] - 101s 817ms/step - loss: 0.8475 - accuracy: 0.8313 - val_loss: 1.2733 - val_accuracy: 0.6992 - lr: 3.6000e-04\n",
      "Epoch 37/140\n",
      "124/124 [==============================] - 98s 788ms/step - loss: 0.8489 - accuracy: 0.8263 - val_loss: 1.3064 - val_accuracy: 0.6996 - lr: 3.6000e-04\n",
      "Epoch 38/140\n",
      "124/124 [==============================] - 98s 791ms/step - loss: 0.8122 - accuracy: 0.8399 - val_loss: 1.0119 - val_accuracy: 0.7762 - lr: 2.1600e-04\n",
      "Epoch 39/140\n",
      "124/124 [==============================] - 97s 785ms/step - loss: 0.7705 - accuracy: 0.8568 - val_loss: 0.9979 - val_accuracy: 0.7916 - lr: 2.1600e-04\n",
      "Epoch 40/140\n",
      "124/124 [==============================] - 100s 806ms/step - loss: 0.7523 - accuracy: 0.8566 - val_loss: 1.1740 - val_accuracy: 0.7406 - lr: 2.1600e-04\n",
      "Epoch 41/140\n",
      "124/124 [==============================] - 99s 798ms/step - loss: 0.7299 - accuracy: 0.8658 - val_loss: 0.9757 - val_accuracy: 0.7981 - lr: 2.1600e-04\n",
      "Epoch 42/140\n",
      "124/124 [==============================] - 98s 793ms/step - loss: 0.7262 - accuracy: 0.8632 - val_loss: 0.9910 - val_accuracy: 0.7933 - lr: 2.1600e-04\n",
      "Epoch 43/140\n",
      "124/124 [==============================] - 98s 787ms/step - loss: 0.7153 - accuracy: 0.8679 - val_loss: 1.0652 - val_accuracy: 0.7702 - lr: 2.1600e-04\n",
      "Epoch 44/140\n",
      "124/124 [==============================] - 98s 794ms/step - loss: 0.6832 - accuracy: 0.8832 - val_loss: 1.0267 - val_accuracy: 0.7839 - lr: 1.2960e-04\n",
      "Epoch 45/140\n",
      "124/124 [==============================] - 98s 792ms/step - loss: 0.6563 - accuracy: 0.8871 - val_loss: 1.0083 - val_accuracy: 0.7831 - lr: 1.2960e-04\n",
      "Epoch 46/140\n",
      "124/124 [==============================] - 99s 796ms/step - loss: 0.6495 - accuracy: 0.8927 - val_loss: 0.9805 - val_accuracy: 0.8042 - lr: 1.2960e-04\n",
      "Epoch 47/140\n",
      "124/124 [==============================] - 100s 807ms/step - loss: 0.6398 - accuracy: 0.8927 - val_loss: 1.0053 - val_accuracy: 0.7876 - lr: 1.2960e-04\n",
      "Epoch 48/140\n",
      "124/124 [==============================] - 108s 872ms/step - loss: 0.6231 - accuracy: 0.8955 - val_loss: 0.9701 - val_accuracy: 0.8103 - lr: 1.2960e-04\n",
      "Epoch 49/140\n",
      "124/124 [==============================] - 109s 876ms/step - loss: 0.6156 - accuracy: 0.8995 - val_loss: 1.1995 - val_accuracy: 0.7434 - lr: 1.2960e-04\n",
      "Epoch 50/140\n",
      "124/124 [==============================] - 98s 789ms/step - loss: 0.5958 - accuracy: 0.9047 - val_loss: 0.9265 - val_accuracy: 0.8135 - lr: 7.7760e-05\n",
      "Epoch 51/140\n",
      "124/124 [==============================] - 97s 778ms/step - loss: 0.5821 - accuracy: 0.9095 - val_loss: 1.0077 - val_accuracy: 0.7912 - lr: 7.7760e-05\n",
      "Epoch 52/140\n",
      "124/124 [==============================] - 98s 787ms/step - loss: 0.5748 - accuracy: 0.9142 - val_loss: 0.9171 - val_accuracy: 0.8204 - lr: 7.7760e-05\n",
      "Epoch 53/140\n",
      "124/124 [==============================] - 100s 809ms/step - loss: 0.5691 - accuracy: 0.9148 - val_loss: 0.9759 - val_accuracy: 0.8107 - lr: 7.7760e-05\n",
      "Epoch 54/140\n",
      "124/124 [==============================] - 99s 795ms/step - loss: 0.5506 - accuracy: 0.9156 - val_loss: 0.9548 - val_accuracy: 0.8066 - lr: 7.7760e-05\n",
      "Epoch 55/140\n",
      "124/124 [==============================] - 96s 771ms/step - loss: 0.5484 - accuracy: 0.9173 - val_loss: 0.9705 - val_accuracy: 0.8107 - lr: 7.7760e-05\n",
      "Epoch 56/140\n",
      "124/124 [==============================] - 93s 749ms/step - loss: 0.5487 - accuracy: 0.9169 - val_loss: 0.9611 - val_accuracy: 0.8026 - lr: 7.7760e-05\n",
      "Epoch 57/140\n",
      "124/124 [==============================] - 116s 934ms/step - loss: 0.5416 - accuracy: 0.9191 - val_loss: 1.0471 - val_accuracy: 0.7815 - lr: 7.7760e-05\n",
      "Epoch 58/140\n",
      "124/124 [==============================] - 122s 985ms/step - loss: 0.5243 - accuracy: 0.9278 - val_loss: 0.9358 - val_accuracy: 0.8111 - lr: 7.7760e-05\n",
      "Epoch 59/140\n",
      "124/124 [==============================] - 97s 779ms/step - loss: 0.5119 - accuracy: 0.9291 - val_loss: 0.9650 - val_accuracy: 0.8095 - lr: 4.6656e-05\n",
      "Epoch 60/140\n",
      "124/124 [==============================] - 92s 743ms/step - loss: 0.5038 - accuracy: 0.9313 - val_loss: 0.9728 - val_accuracy: 0.8071 - lr: 4.6656e-05\n",
      "Epoch 61/140\n",
      "124/124 [==============================] - 92s 743ms/step - loss: 0.5009 - accuracy: 0.9307 - val_loss: 0.9516 - val_accuracy: 0.8075 - lr: 4.6656e-05\n",
      "Epoch 62/140\n",
      "124/124 [==============================] - 100s 805ms/step - loss: 0.4958 - accuracy: 0.9335 - val_loss: 1.0212 - val_accuracy: 0.7896 - lr: 4.6656e-05\n",
      "Epoch 63/140\n",
      "124/124 [==============================] - 102s 825ms/step - loss: 0.4961 - accuracy: 0.9353 - val_loss: 1.0238 - val_accuracy: 0.7985 - lr: 4.6656e-05\n",
      "Epoch 64/140\n",
      "124/124 [==============================] - 104s 840ms/step - loss: 0.4856 - accuracy: 0.9345 - val_loss: 0.9702 - val_accuracy: 0.8083 - lr: 4.6656e-05\n",
      "Epoch 65/140\n",
      "124/124 [==============================] - 106s 847ms/step - loss: 0.4860 - accuracy: 0.9343 - val_loss: 0.9628 - val_accuracy: 0.8095 - lr: 2.7994e-05\n",
      "Epoch 66/140\n",
      "124/124 [==============================] - 114s 917ms/step - loss: 0.4728 - accuracy: 0.9407 - val_loss: 0.9783 - val_accuracy: 0.8083 - lr: 2.7994e-05\n",
      "Epoch 67/140\n",
      "124/124 [==============================] - 91s 727ms/step - loss: 0.4712 - accuracy: 0.9393 - val_loss: 0.9323 - val_accuracy: 0.8160 - lr: 2.7994e-05\n",
      "Epoch 68/140\n",
      "124/124 [==============================] - 85s 680ms/step - loss: 0.4694 - accuracy: 0.9411 - val_loss: 0.9633 - val_accuracy: 0.8087 - lr: 2.7994e-05\n",
      "Epoch 69/140\n",
      "124/124 [==============================] - 83s 666ms/step - loss: 0.4663 - accuracy: 0.9424 - val_loss: 0.9463 - val_accuracy: 0.8156 - lr: 2.7994e-05\n",
      "Epoch 70/140\n",
      "124/124 [==============================] - 82s 661ms/step - loss: 0.4642 - accuracy: 0.9416 - val_loss: 0.9765 - val_accuracy: 0.8062 - lr: 2.7994e-05\n",
      "Epoch 71/140\n",
      "124/124 [==============================] - 82s 665ms/step - loss: 0.4575 - accuracy: 0.9435 - val_loss: 0.9396 - val_accuracy: 0.8192 - lr: 1.6796e-05\n",
      "Epoch 72/140\n",
      "124/124 [==============================] - 83s 672ms/step - loss: 0.4537 - accuracy: 0.9462 - val_loss: 0.9860 - val_accuracy: 0.8103 - lr: 1.6796e-05\n",
      "Epoch 73/140\n",
      "124/124 [==============================] - 81s 653ms/step - loss: 0.4558 - accuracy: 0.9456 - val_loss: 0.9907 - val_accuracy: 0.8103 - lr: 1.6796e-05\n",
      "Epoch 74/140\n",
      "124/124 [==============================] - 82s 665ms/step - loss: 0.4438 - accuracy: 0.9498 - val_loss: 0.9483 - val_accuracy: 0.8131 - lr: 1.6796e-05\n",
      "Epoch 75/140\n",
      "124/124 [==============================] - 83s 670ms/step - loss: 0.4419 - accuracy: 0.9495 - val_loss: 0.9553 - val_accuracy: 0.8111 - lr: 1.6796e-05\n",
      "Epoch 76/140\n",
      "124/124 [==============================] - 82s 664ms/step - loss: 0.4445 - accuracy: 0.9473 - val_loss: 0.9954 - val_accuracy: 0.8042 - lr: 1.6796e-05\n",
      "Epoch 77/140\n",
      "124/124 [==============================] - 83s 672ms/step - loss: 0.4485 - accuracy: 0.9458 - val_loss: 0.9395 - val_accuracy: 0.8184 - lr: 1.0078e-05\n",
      "Epoch 78/140\n",
      "124/124 [==============================] - 83s 667ms/step - loss: 0.4362 - accuracy: 0.9479 - val_loss: 0.9918 - val_accuracy: 0.8030 - lr: 1.0078e-05\n",
      "Epoch 79/140\n",
      "124/124 [==============================] - 83s 667ms/step - loss: 0.4414 - accuracy: 0.9493 - val_loss: 0.9829 - val_accuracy: 0.8030 - lr: 1.0078e-05\n",
      "Epoch 80/140\n",
      "124/124 [==============================] - 84s 679ms/step - loss: 0.4415 - accuracy: 0.9477 - val_loss: 0.9610 - val_accuracy: 0.8099 - lr: 1.0078e-05\n",
      "Epoch 81/140\n",
      "124/124 [==============================] - 82s 664ms/step - loss: 0.4279 - accuracy: 0.9565 - val_loss: 0.9782 - val_accuracy: 0.8058 - lr: 1.0078e-05\n",
      "Epoch 82/140\n",
      "124/124 [==============================] - 82s 662ms/step - loss: 0.4391 - accuracy: 0.9495 - val_loss: 0.9566 - val_accuracy: 0.8103 - lr: 1.0078e-05\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.9171 - accuracy: 0.8204\n",
      "Validation Loss: 0.9171, Accuracy: 0.8204\n",
      "3/3 [==============================] - 2s 231ms/step\n",
      "Sample Predictions:\n",
      "('living room', 'living room')\n",
      "('bath', 'bath')\n",
      "('bath', 'bath')\n",
      "('bath', 'bath')\n",
      "('bed', 'bed')\n",
      "('bath', 'bed')\n",
      "('dining room', 'dining room')\n",
      "('kitchen', 'kitchen')\n",
      "('bed', 'bed')\n",
      "('bed', 'bed')\n",
      "31/31 [==============================] - 2s 28ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        bath       0.86      0.90      0.88       486\n",
      "         bed       0.75      0.88      0.81       489\n",
      " dining room       0.84      0.77      0.81       521\n",
      "     kitchen       0.88      0.80      0.83       447\n",
      " living room       0.80      0.76      0.78       524\n",
      "\n",
      "    accuracy                           0.82      2467\n",
      "   macro avg       0.82      0.82      0.82      2467\n",
      "weighted avg       0.82      0.82      0.82      2467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Dense, Dropout, BatchNormalization, \n",
    "    GlobalAveragePooling2D, RandomFlip, RandomRotation\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set seeds for reproducibility to ensure consistent results across runs\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths for dataset\n",
    "CSV_PATH = \"../Dataset/image_labels.csv\"  # CSV file containing image paths and labels\n",
    "IMG_DIR = \"../Dataset/interior\"           # Directory containing the images\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224          # Image dimensions for resizing\n",
    "BATCH_SIZE = 80                           # Batch size for training\n",
    "EPOCHS = 140                              # Maximum number of epochs for training\n",
    "\n",
    "# Generate a CSV file mapping image paths to their labels\n",
    "# This function scans the image directory, matches filenames to class labels, and saves the mapping to a CSV\n",
    "def regenerate_csv(image_dir, output_file):\n",
    "    # Define the classes we want to classify\n",
    "    classes = ['bath', 'bed', 'dining room', 'kitchen', 'living room']\n",
    "    # Variations of class names to handle different filename patterns\n",
    "    class_variations = {\n",
    "        'bath': ['bath', 'bathroom'], 'bed': ['bed', 'bedroom'],\n",
    "        'dining room': ['dining', 'dining_room', 'diningroom', 'din'],\n",
    "        'kitchen': ['kitchen'], 'living room': ['living', 'living_room', 'livingroom']\n",
    "    }\n",
    "    data = []\n",
    "    # Iterate through all files in the image directory\n",
    "    for filename in os.listdir(image_dir):\n",
    "        # Check if the file is an image (jpg, jpeg, or png)\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            matched = False\n",
    "            # Try to match the filename to a class based on variations\n",
    "            for cls in classes:\n",
    "                for variation in class_variations[cls]:\n",
    "                    if variation.lower() in filename.lower():\n",
    "                        # If matched, add the image path and label to the data list\n",
    "                        data.append({'image_path': os.path.join(image_dir, filename), 'label': cls})\n",
    "                        matched = True\n",
    "                        break\n",
    "                if matched:\n",
    "                    break\n",
    "    # Convert the data list to a DataFrame and save it as a CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Regenerated CSV with {len(df)} images\")\n",
    "    return df\n",
    "\n",
    "# Verify that all image paths in the CSV exist\n",
    "def verify_data(csv_path, img_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Total images: {len(df)}, Classes: {df['label'].value_counts()}\")\n",
    "    # Check for missing files\n",
    "    missing = [path for path in df['image_path'] if not os.path.exists(path)]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing files: {missing}\")\n",
    "\n",
    "# Load and preprocess the dataset for training\n",
    "def load_and_preprocess_data(csv_path, img_dir):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Encode the string labels (e.g., 'bath') into integers (e.g., 0, 1, ...)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "    num_classes = len(label_encoder.classes_)  # Number of unique classes (should be 5)\n",
    "\n",
    "    # Split the data into training (80%) and validation (20%) sets, ensuring class balance\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "    \n",
    "    # Compute class weights to handle slight imbalances in the dataset\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(df['label_encoded']), y=df['label_encoded'])\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    # Helper function to normalize images \n",
    "    def load_image(image_path, label):\n",
    "        # Read the image file\n",
    "        img = tf.io.read_file(image_path)\n",
    "        # Decode the image as JPEG with 3 color channels (RGB)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        # Convert the pixel values to float32\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        # Normalize pixel values to [0, 1]\n",
    "        img = img / 255.0\n",
    "        return img, label\n",
    "\n",
    "    # Create TensorFlow datasets for training and validation\n",
    "    # Map the image loading function, cache data in memory, shuffle, batch, and prefetch for efficiency\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (train_df['image_path'], tf.keras.utils.to_categorical(train_df['label_encoded'], num_classes))\n",
    "    ).map(load_image, num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (val_df['image_path'], tf.keras.utils.to_categorical(val_df['label_encoded'], num_classes))\n",
    "    ).map(load_image, num_parallel_calls=tf.data.AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset, num_classes, label_encoder, val_df, class_weight_dict\n",
    "\n",
    "# Build the Model\n",
    "def build_model(num_classes):\n",
    "    # Initialize a sequential model (stack of layers)\n",
    "    model = Sequential([\n",
    "        # Data Augmentation Layers: These are applied during training to prevent overfitting\n",
    "        # Randomly flip images horizontally to increase dataset variability\n",
    "        RandomFlip(\"horizontal\", seed=42),\n",
    "        # Randomly rotate images by up to 10% (0.1 radians) to make the model robust to rotations\n",
    "        RandomRotation(0.1, seed=42),\n",
    "\n",
    "        # First Convolutional Block\n",
    "        # Conv2D: Apply 32 filters of size 3x3, with 'same' padding to maintain input dimensions\n",
    "        # Activation: ReLU introduces non-linearity\n",
    "        # Input Shape: Images are 224x224 with 3 color channels (RGB)\n",
    "        # L2 Regularization: Penalizes large weights to prevent overfitting\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), kernel_regularizer=l2(0.001)),\n",
    "        # BatchNormalization: Normalizes layer outputs to stabilize and accelerate training\n",
    "        BatchNormalization(),\n",
    "        # MaxPooling2D: Downsamples the feature maps by taking the maximum value in 2x2 regions, reducing spatial dimensions\n",
    "        MaxPooling2D((2, 2)),\n",
    "        # Dropout: Randomly sets 45% of the units to 0 during training to prevent overfitting\n",
    "        Dropout(0.45),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        # Increase to 64 filters to capture more complex features\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.45),\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        # Increase to 128 filters for deeper feature extraction\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.45),\n",
    "\n",
    "        # Fourth Convolutional Block\n",
    "        # Increase to 256 filters to capture high-level features\n",
    "        Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.45),\n",
    "\n",
    "        # Fifth and Sixth Convolutional Blocks\n",
    "        # Two consecutive Conv2D layers with 256 filters to deepen the network\n",
    "        Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # Global Average Pooling: Reduces spatial dimensions (e.g., 14x14x256) to a 1D vector (256,)\n",
    "        # This avoids the need for flattening and reduces the number of parameters\n",
    "        GlobalAveragePooling2D(),\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        # Dense layer with 256 units to learn complex patterns from the pooled features\n",
    "        # L2 regularization to prevent overfitting\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        # High dropout rate (70%) to further combat overfitting\n",
    "        Dropout(0.7),\n",
    "\n",
    "        # Output Layer\n",
    "        # Dense layer with 'num_classes' units (5 in this case) for classification\n",
    "        # Softmax activation to output probabilities for each class\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Plot the training history (accuracy and loss)\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for metric in ['accuracy', 'loss']:\n",
    "        plt.subplot(1, 2, 1 if metric == 'accuracy' else 2)\n",
    "        plt.plot(history.history[metric], label=f'Training {metric.capitalize()}')\n",
    "        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric.capitalize()}')\n",
    "        plt.title(f'Training and Validation {metric.capitalize()}')\n",
    "        plt.legend()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot the confusion matrix to visualize model performance\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main function to execute the pipeline\n",
    "def main():\n",
    "    # Generate the CSV file with image paths and labels\n",
    "    df = regenerate_csv(IMG_DIR, CSV_PATH)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Verify that all image paths exist\n",
    "    verify_data(CSV_PATH, IMG_DIR)\n",
    "    \n",
    "    # Load and preprocess the dataset\n",
    "    train_dataset, val_dataset, num_classes, label_encoder, val_df, class_weight_dict = load_and_preprocess_data(CSV_PATH, IMG_DIR)\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_model(num_classes)\n",
    "    \n",
    "    # Compile the model\n",
    "    # Optimizer: Adam with a learning rate of 0.001\n",
    "    # Loss: Categorical crossentropy for multi-class classification\n",
    "    # Metrics: Track accuracy during training\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks to improve training\n",
    "    callbacks = [\n",
    "        # EarlyStopping: Stop training if validation loss doesn't improve for 30 epochs\n",
    "        # Restore the best weights to avoid overfitting\n",
    "        EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),\n",
    "        # ReduceLROnPlateau: Reduce learning rate by a factor of 0.6 if validation loss plateaus for 6 epochs\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=6, min_lr=1e-6),\n",
    "        # CSVLogger: Log training metrics (e.g., loss, accuracy) to a CSV file\n",
    "        CSVLogger('training_log.csv', append=True)\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    # Use the training dataset, validate on the validation dataset\n",
    "    # Apply class weights to handle imbalance\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=EPOCHS, validation_data=val_dataset,\n",
    "        callbacks=callbacks, class_weight=class_weight_dict, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    model.save('model.keras')\n",
    "    \n",
    "    # Save the label encoder classes for later use\n",
    "    np.save('label_encoder_classes.npy', label_encoder.classes_)\n",
    "    \n",
    "    # Plot training and validation accuracy/loss\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Get sample predictions to inspect model performance\n",
    "    val_images, val_labels = next(iter(val_dataset))\n",
    "    predictions = model.predict(val_images)\n",
    "    predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    true_labels = label_encoder.inverse_transform(np.argmax(val_labels, axis=1))\n",
    "    print(\"Sample Predictions:\", *[(t, p) for t, p in zip(true_labels[:10], predicted_labels[:10])], sep='\\n')\n",
    "    \n",
    "    # Generate a classification report and confusion matrix\n",
    "    val_predictions = model.predict(val_dataset)\n",
    "    val_pred_labels = np.argmax(val_predictions, axis=1)\n",
    "    val_true_labels = np.argmax(np.concatenate([y for _, y in val_dataset]), axis=1)\n",
    "    report = classification_report(val_true_labels, val_pred_labels, target_names=label_encoder.classes_)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    \n",
    "    # Save the classification report to a file\n",
    "    with open('classification_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # Plot and save the confusion matrix\n",
    "    plot_confusion_matrix(val_true_labels, val_pred_labels, label_encoder.classes_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
