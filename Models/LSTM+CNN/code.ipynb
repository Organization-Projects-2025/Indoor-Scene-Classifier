{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae07bb0-ad5f-49a1-9185-d2ccacdc975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8979\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        bath       0.93      0.95      0.94       486\n",
      "         bed       0.92      0.91      0.91       489\n",
      " dining room       0.90      0.88      0.89       521\n",
      "     kitchen       0.92      0.87      0.89       447\n",
      " living room       0.83      0.88      0.86       524\n",
      "\n",
      "    accuracy                           0.90      2467\n",
      "   macro avg       0.90      0.90      0.90      2467\n",
      "weighted avg       0.90      0.90      0.90      2467\n",
      "\n",
      "SVM model saved as 'svm_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Dense, Dropout, BatchNormalization,\n",
    "    GlobalAveragePooling2D, RandomFlip, RandomRotation, RandomZoom, RandomContrast,\n",
    "    LSTM, Reshape, Input\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Enable XLA for optimization\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable mixed precision\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "# Paths and hyperparameters\n",
    "CSV_PATH = r\"C:\\Users\\ahmed\\Downloads\\ML-Project\\Dataset\\image_labels.csv\"\n",
    "IMG_DIR = r\"C:\\Users\\ahmed\\Downloads\\ML-Project\\Dataset\\interior\"\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "BATCH_SIZE = 32  # Reduced for better generalization\n",
    "EPOCHS = 200\n",
    "\n",
    "def initialize_environment():\n",
    "    \"\"\"Log environment details and initialize settings.\"\"\"\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "def regenerate_csv(image_dir, output_file):\n",
    "    \"\"\"\n",
    "    Scans the image directory, matches filenames to class labels based on predefined variations,\n",
    "    and saves the mapping to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        image_dir (str): Directory containing the images.\n",
    "        output_file (str): Path to save the generated CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing image paths and labels.\n",
    "    \"\"\"\n",
    "    classes = ['bath', 'bed', 'dining room', 'kitchen', 'living room']\n",
    "    class_variations = {\n",
    "        'bath': ['bath', 'bathroom'], 'bed': ['bed', 'bedroom'],\n",
    "        'dining room': ['dining', 'dining_room', 'diningroom', 'din'],\n",
    "        'kitchen': ['kitchen'], 'living room': ['living', 'living_room', 'livingroom']\n",
    "    }\n",
    "    data = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            matched = False\n",
    "            for cls in classes:\n",
    "                for variation in class_variations[cls]:\n",
    "                    if variation.lower() in filename.lower():\n",
    "                        img_path = os.path.abspath(os.path.join(image_dir, filename))\n",
    "                        data.append({'image_path': img_path, 'label': cls})\n",
    "                        matched = True\n",
    "                        break\n",
    "                if matched:\n",
    "                    break\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    logging.info(f\"Regenerated CSV with {len(df)} images\")\n",
    "    return df\n",
    "\n",
    "def validate_images(df, img_dir):\n",
    "    \"\"\"\n",
    "    Validate image paths and log errors for invalid images.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing image paths and labels.\n",
    "        img_dir (str): Directory containing the images.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of invalid image paths.\n",
    "    \"\"\"\n",
    "    invalid_images = []\n",
    "    for img_path in df['image_path']:\n",
    "        img_path = img_path.replace('/', '\\\\')  # Normalize for Windows\n",
    "        if not os.path.exists(img_path):\n",
    "            invalid_images.append(img_path)\n",
    "            logging.error(f\"Image not found: {img_path}\")\n",
    "            continue\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img.verify()\n",
    "        except Exception as e:\n",
    "            invalid_images.append(img_path)\n",
    "            logging.error(f\"Invalid image {img_path}: {e}\")\n",
    "    return invalid_images\n",
    "\n",
    "def verify_data(csv_path, img_dir):\n",
    "    \"\"\"\n",
    "    Verify that all image paths in the CSV file exist.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        img_dir (str): Directory containing the images.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any image file is missing.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Total images: {len(df)}, Classes: {df['label'].value_counts()}\")\n",
    "    missing = [path for path in df['image_path'] if not os.path.exists(path)]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing files: {missing}\")\n",
    "\n",
    "def load_and_preprocess_data(csv_path, img_dir):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset with error handling.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        img_dir (str): Directory containing the images.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset, num_classes, label_encoder, val_df, class_weight_dict)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(df['label_encoded']), y=df['label_encoded'])\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    def load_image(image_path, label):\n",
    "        try:\n",
    "            img = tf.io.read_file(image_path)\n",
    "            img = tf.cond(tf.strings.length(img) == 0,\n",
    "                          lambda: tf.zeros([IMG_HEIGHT, IMG_WIDTH, 3], tf.float32),\n",
    "                          lambda: tf.image.resize(tf.image.decode_jpeg(img, channels=3), [IMG_HEIGHT, IMG_WIDTH]))\n",
    "            img = tf.cast(img, tf.float32)\n",
    "            return img, label\n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "            logging.error(f\"Failed to load image {image_path}: {e}\")\n",
    "            return tf.zeros([IMG_HEIGHT, IMG_WIDTH, 3], tf.float32), label\n",
    "\n",
    "    def filter_valid_samples(image, label):\n",
    "        return tf.reduce_any(tf.not_equal(image, tf.zeros([IMG_HEIGHT, IMG_WIDTH, 3])))\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (train_df['image_path'].values, tf.keras.utils.to_categorical(train_df['label_encoded'], num_classes))\n",
    "    ).map(load_image, num_parallel_calls=tf.data.AUTOTUNE).filter(filter_valid_samples).cache().shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (val_df['image_path'].values, tf.keras.utils.to_categorical(val_df['label_encoded'], num_classes))\n",
    "    ).map(load_image, num_parallel_calls=tf.data.AUTOTUNE).filter(filter_valid_samples).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_dataset, val_dataset, num_classes, label_encoder, val_df, class_weight_dict\n",
    "\n",
    "def build_cnn_lstm_model(num_classes):\n",
    "    \"\"\"\n",
    "    Build a CNN-LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): Number of output classes.\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled CNN-LSTM model.\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    x = input_layer\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, 256))(x)\n",
    "    x = LSTM(512, return_sequences=False, dropout=0.3, recurrent_dropout=0.2)(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "    x = Dropout(0.65)(x)\n",
    "    output = Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    return model\n",
    "\n",
    "def compile_and_train_model(model, train_dataset, val_dataset, num_classes, class_weight_dict):\n",
    "    \"\"\"\n",
    "    Compile and train the CNN-LSTM model with specified callbacks.\n",
    "    \n",
    "    Args:\n",
    "        model: The CNN-LSTM model to train.\n",
    "        train_dataset: Training dataset.\n",
    "        val_dataset: Validation dataset.\n",
    "        num_classes (int): Number of output classes.\n",
    "        class_weight_dict (dict): Class weights for training.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, history)\n",
    "    \"\"\"\n",
    "    learning_rate = 5e-4\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.build((None, IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    model.summary()\n",
    "\n",
    "    class TimeHistory(tf.keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.times = []\n",
    "        def on_epoch_begin(self, epoch, logs={}):\n",
    "            self.epoch_time_start = time.time()\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=7, min_lr=1e-6),\n",
    "        TensorBoard(log_dir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "        CSVLogger('training_log_cnn_lstm.csv', append=True),\n",
    "        TimeHistory()\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=EPOCHS, validation_data=val_dataset,\n",
    "        callbacks=callbacks, class_weight=class_weight_dict, verbose=1\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, val_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        val_dataset: Validation dataset.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (val_loss, val_accuracy)\n",
    "    \"\"\"\n",
    "    val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def save_model_and_metadata(model, label_encoder, history_callback):\n",
    "    \"\"\"\n",
    "    Save the trained model and related metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        label_encoder: Label encoder object.\n",
    "        history_callback: Callback containing epoch times.\n",
    "    \"\"\"\n",
    "    model.save('model_cnn_lstm.keras')\n",
    "    np.save('label_encoder_classes_cnn_lstm.npy', label_encoder.classes_)\n",
    "\n",
    "    with open('epoch_times_cnn_lstm.txt', 'w') as f:\n",
    "        f.write(\"Epoch Times (seconds):\\n\" + \"\\n\".join(map(str, history_callback.times)))\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot and save training history.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history object.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for metric in ['accuracy', 'loss']:\n",
    "        plt.subplot(1, 2, 1 if metric == 'accuracy' else 2)\n",
    "        plt.plot(history.history[metric], label=f'Training {metric.capitalize()}')\n",
    "        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric.capitalize()}')\n",
    "        plt.title(f'Training and Validation {metric.capitalize()}')\n",
    "        plt.legend()\n",
    "    plt.savefig('training_history_cnn_lstm.png')\n",
    "    plt.close()\n",
    "\n",
    "def generate_and_evaluate_predictions(model, val_dataset, label_encoder, val_df):\n",
    "    \"\"\"\n",
    "    Generate predictions and evaluate model performance.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model.\n",
    "        val_dataset: Validation dataset.\n",
    "        label_encoder: Label encoder object.\n",
    "        val_df: Validation DataFrame.\n",
    "    \"\"\"\n",
    "    val_images, val_labels = next(iter(val_dataset))\n",
    "    predictions = model.predict(val_images)\n",
    "    predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    true_labels = label_encoder.inverse_transform(np.argmax(val_labels, axis=1))\n",
    "    print(\"Sample Predictions:\", *[(t, p) for t, p in zip(true_labels[:10], predicted_labels[:10])], sep='\\n')\n",
    "\n",
    "    val_predictions = model.predict(val_dataset)\n",
    "    val_pred_labels = np.argmax(val_predictions, axis=1)\n",
    "    val_true_labels = np.argmax(np.concatenate([y for _, y in val_dataset]), axis=1)\n",
    "    report = classification_report(val_true_labels, val_pred_labels, target_names=label_encoder.classes_)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    with open('classification_report_cnn_lstm.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    plot_confusion_matrix(val_true_labels, val_pred_labels, label_encoder.classes_)\n",
    "\n",
    "    val_images_paths = val_df['image_path'].values\n",
    "    misclassified = [(t, p, img) for t, p, img in zip(label_encoder.inverse_transform(val_true_labels),\n",
    "                                                     label_encoder.inverse_transform(val_pred_labels),\n",
    "                                                     val_images_paths) if t != p]\n",
    "    print(f\"Misclassified examples: {len(misclassified)}\")\n",
    "    with open('misclassified_images_cnn_lstm.txt', 'w') as f:\n",
    "        for t, p, img in misclassified[:10]:\n",
    "            f.write(f\"True: {t}, Predicted: {p}, Image: {img}\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Execute the full CNN-LSTM pipeline.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize environment\n",
    "    initialize_environment()\n",
    "\n",
    "    # Step 2: Regenerate CSV\n",
    "    df = regenerate_csv(IMG_DIR, CSV_PATH)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # Step 3: Validate images\n",
    "    invalid_images = validate_images(df, IMG_DIR)\n",
    "    if invalid_images:\n",
    "        logging.warning(f\"Found {len(invalid_images)} invalid images. Removing them.\")\n",
    "        df = df[~df['image_path'].isin(invalid_images)]\n",
    "        df.to_csv(CSV_PATH, index=False)\n",
    "        print(f\"Updated CSV with {len(df)} valid images\")\n",
    "\n",
    "    # Step 4: Verify data\n",
    "    verify_data(CSV_PATH, IMG_DIR)\n",
    "\n",
    "    # Step 5: Load and preprocess data\n",
    "    train_dataset, val_dataset, num_classes, label_encoder, val_df, class_weight_dict = load_and_preprocess_data(CSV_PATH, IMG_DIR)\n",
    "\n",
    "    # Step 6: Build and train model\n",
    "    model = build_cnn_lstm_model(num_classes)\n",
    "    model, history = compile_and_train_model(model, train_dataset, val_dataset, num_classes, class_weight_dict)\n",
    "\n",
    "    # Step 7: Evaluate model\n",
    "    evaluate_model(model, val_dataset)\n",
    "\n",
    "    # Step 8: Save model and metadata\n",
    "    save_model_and_metadata(model, label_encoder, history.callbacks[-1])\n",
    "\n",
    "    # Step 9: Plot training history\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Step 10: Generate and evaluate predictions\n",
    "    generate_and_evaluate_predictions(model, val_dataset, label_encoder, val_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
